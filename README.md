---
title: "AI Novelist RAG"
emoji: "ğŸ“š"
colorFrom: "indigo"
colorTo: "purple"
sdk: docker
sdk_version: 28.0.4
python_version: 3.10.16
app_port: 7860
accelerator: cpu # gpu when local, be online in future
---


# AI Novelist RAG - Long-Form Story Generation with Memory

This is a dedicated AI system for generating long-form novels with coherent logic, consistent world-building, and thematic integrity, solving common LLMs issues like incoherence, self-contradiction, and theme drift.

This project combines Langchain, Retrieval-Augmented Generation (RAG), FAISS, FastAPI, and Gradio, along with OpenAI or local quantized models (e.g., LLaMA). It allows the AI to extract key information and context from its own outputs and store them in a vector database to â€œrememberâ€ what itâ€™s writtenâ€”resulting in more logical and immersive storytelling. 

This system is designed as both a technical practice and a creative tool. 

## Key Features

- High-quality novel generation (via OpenAI APIs or local LLMs)
- Automatic information extraction (currently via BERT-based summary model)
- Long-context memory (via FAISS vector store)
- Context-aware coherent prompt (via RAG-like context enhancement)
- Web-based UI (via Gradio)
- Experimental consistency scoring (planned to use long-context metrics)

## Project Structure

```
ai-novelist-rag/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                # FastAPI + Gradio mount point
â”‚   â”œâ”€â”€ apis/                  # API endpoints for generation, editing, benchmarks
â”‚   â”œâ”€â”€ front_end/             # Gradio-based UI
â”‚   â”œâ”€â”€ managers/              # Managers and chains for chapter, summary, and vector memory
â”‚   â”œâ”€â”€ models/                # Model loading and wrapper logic
â”‚   â”œâ”€â”€ tests/                 # (Not showed here) Jupyter notebooks for quick testing
â”‚   â””â”€â”€ utils/                 # General utilities 
â”‚
â”œâ”€â”€ data/                      # (Not showed here) Local store for texts, summaries, vector DB 
â”œâ”€â”€ docker/                    # Dockerfile and config for deployment
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.sh               # Install dependencies
â”‚   â””â”€â”€ start.sh               # Start the app program
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ Dockerfile
â””â”€â”€ .gitignore
```

## Getting Started

1. Install dependencies:

```bash
bash scripts/setup.sh
```

2. Start the app program:

```bash
bash scripts/start.sh
```

Default setup:

- Uses gpt-4o-mini (via OpenAI API) for novel generation
- Uses bart-large-cnn for chapter summarization  
- Configurable in config.py (coming soon)

Requires your own OpenAI API key

## Coming Soon

- Chinese-style writing support
- Multiple books/novels within the same workspace
- Streaming generation responses to frontend
- Benchmark consistency improvement with and without memory
- Deployment to Azure

## Contributing

This project is still under active development, Star, Fork, and PRs are welcome!
